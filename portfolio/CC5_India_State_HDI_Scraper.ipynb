{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CC5: Web Scraping - India State-wise Development Data\n",
    "\n",
    "**Objective**: Scrape India's state-wise Human Development Index (HDI) data from Wikipedia and convert it to tidy (long) format for visualization.\n",
    "\n",
    "**Data Source**: Wikipedia - List of Indian states and union territories by Human Development Index\n",
    "\n",
    "**Tools**: BeautifulSoup, Pandas, Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install beautifulsoup4 pandas requests lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Scrape the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target URL\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_Indian_states_and_union_territories_by_Human_Development_Index'\n",
    "\n",
    "# Send GET request\n",
    "response = requests.get(url)\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "\n",
    "# Parse HTML\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "print(\"Successfully parsed HTML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the table with HDI data\n",
    "# Looking for tables with class 'wikitable'\n",
    "tables = soup.find_all('table', {'class': 'wikitable'})\n",
    "print(f\"Found {len(tables)} tables\")\n",
    "\n",
    "# Display first few rows of the first table to verify\n",
    "if tables:\n",
    "    df_raw = pd.read_html(str(tables[0]))[0]\n",
    "    print(\"\\nFirst table preview:\")\n",
    "    print(df_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clean and Normalize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the relevant table (adjust index if needed based on preview)\n",
    "df = pd.read_html(str(tables[0]))[0]\n",
    "\n",
    "# Display raw data structure\n",
    "print(\"Raw columns:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nRaw data shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean column names - remove multi-level headers if present\n",
    "if isinstance(df.columns, pd.MultiIndex):\n",
    "    df.columns = ['_'.join(col).strip() for col in df.columns.values]\n",
    "\n",
    "# Rename columns for clarity (adjust based on actual column names)\n",
    "# Typical structure: Rank, State/UT, HDI value, etc.\n",
    "\n",
    "# Select relevant columns - adjust based on your data\n",
    "# Example: keeping state name and HDI value columns\n",
    "print(\"\\nCleaned columns:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "# Remove any rows with missing values\n",
    "df_clean = df.dropna(subset=[df.columns[1], df.columns[2]])  # Adjust column indices\n",
    "\n",
    "# Remove footnote markers and special characters\n",
    "# Clean state names\n",
    "if 'State' in df_clean.columns or 'State/UT' in df_clean.columns:\n",
    "    state_col = 'State' if 'State' in df_clean.columns else 'State/UT'\n",
    "    df_clean[state_col] = df_clean[state_col].str.replace(r'\\[.*?\\]', '', regex=True)\n",
    "    df_clean[state_col] = df_clean[state_col].str.strip()\n",
    "\n",
    "print(\"\\nCleaned data:\")\n",
    "print(df_clean.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Convert to TIDY (Long) Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simplified tidy dataset\n",
    "# Select key columns: State, HDI, Rank\n",
    "\n",
    "# Adjust column selection based on actual data structure\n",
    "tidy_df = df_clean.iloc[:, [1, 2]].copy()  # Typically: State name and HDI value\n",
    "tidy_df.columns = ['state', 'hdi']  # Standardize column names\n",
    "\n",
    "# Convert HDI to numeric, handling any text\n",
    "tidy_df['hdi'] = pd.to_numeric(tidy_df['hdi'], errors='coerce')\n",
    "\n",
    "# Remove any remaining null values\n",
    "tidy_df = tidy_df.dropna()\n",
    "\n",
    "# Add year column (metadata)\n",
    "tidy_df['year'] = 2021  # Adjust based on Wikipedia data year\n",
    "\n",
    "# Sort by HDI value descending\n",
    "tidy_df = tidy_df.sort_values('hdi', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nTIDY FORMAT DATA:\")\n",
    "print(tidy_df.head(15))\n",
    "print(f\"\\nTotal states/UTs: {len(tidy_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Validate Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data quality\n",
    "print(\"Data Quality Checks:\")\n",
    "print(f\"- Missing values: {tidy_df.isnull().sum().sum()}\")\n",
    "print(f\"- HDI range: {tidy_df['hdi'].min():.3f} to {tidy_df['hdi'].max():.3f}\")\n",
    "print(f\"- Data types:\\n{tidy_df.dtypes}\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(tidy_df['hdi'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Export to CSV (Tidy Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "tidy_df.to_csv('india_state_hdi.csv', index=False)\n",
    "print(\"\\n✅ Data exported to 'india_state_hdi.csv'\")\n",
    "\n",
    "# Display final dataset\n",
    "print(\"\\nFinal TIDY dataset:\")\n",
    "print(tidy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file\n",
    "from google.colab import files\n",
    "files.download('india_state_hdi.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we did:**\n",
    "1. Scraped Wikipedia table containing India's state-wise HDI data\n",
    "2. Cleaned data: removed footnotes, special characters, and null values\n",
    "3. Normalized to tidy format: each row = one observation (state), each column = one variable\n",
    "4. Exported as CSV for use in Vega-Lite visualization\n",
    "\n",
    "**Challenges:**\n",
    "- Wikipedia tables often have multi-level headers requiring careful parsing\n",
    "- Footnote markers and special characters needed removal\n",
    "- Converting scraped text to proper numeric format for analysis\n",
    "\n",
    "**Tidy Data Principles Applied:**\n",
    "- ✅ Each variable forms a column (state, hdi, year)\n",
    "- ✅ Each observation forms a row (one state per row)\n",
    "- ✅ Ready for visualization in Vega-Lite"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
